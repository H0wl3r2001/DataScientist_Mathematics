{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [**Eigenvalues and Eigenvectors**](#eigen)\n",
    "2. [**PCA in Python**](#PCA)\n",
    "3. [**Importance and limitations**](#limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors <a class=\"anchor\" id=\"eigen\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are fundamental concepts in linear algebra with significant applications in machine learning and data science.\n",
    "\n",
    "Given a square matrix **A**, an eigenvector **v** and an eigenvalue **λ** satisfy the equation:\n",
    "\n",
    "$$ A \\mathbf{v} = \\lambda \\mathbf{v} $$\n",
    "\n",
    "- **Eigenvector**: A non-zero vector **v** that only changes by a scalar factor when the matrix **A** is applied to it.\n",
    "- **Eigenvalue**: The scalar **λ** that represents how much the eigenvector is scaled during the transformation.\n",
    "\n",
    "### Applications of Eigenvectors and Eigenvalues in Data Science\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**:\n",
    "   - PCA is a dimensionality reduction technique that transforms data into a set of orthogonal (uncorrelated) components.\n",
    "   - It uses eigenvalues and eigenvectors of the covariance matrix of the data to identify the principal components.\n",
    "   - The eigenvectors represent the directions of maximum variance, and the eigenvalues indicate the magnitude of variance in these directions.\n",
    "\n",
    "2. **Feature Reduction**:\n",
    "   - By selecting the top eigenvectors (principal components) with the largest eigenvalues, we can reduce the number of features while retaining most of the data's variability.\n",
    "   - This helps in reducing computational cost and avoiding overfitting.\n",
    "\n",
    "3. **Stability and Dynamics**:\n",
    "   - In systems modeled by differential equations, eigenvalues can indicate stability. For instance, in Markov chains, eigenvalues help understand the long-term behavior of the system.\n",
    "\n",
    "4. **Graph Theory**:\n",
    "   - Eigenvalues and eigenvectors of adjacency matrices or Laplacian matrices of graphs are used in spectral clustering, which is a technique for identifying communities within a graph.\n",
    "\n",
    "5. **Data Transformation**:\n",
    "   - Eigenvectors can be used to transform data into a new coordinate system, simplifying the problem and making patterns more apparent.\n",
    "\n",
    "## PCA in Python <a class=\"anchor\" id=\"PCA\"></a>\n",
    "\n",
    "Here's a simple example of performing PCA using Python's `numpy` and `scikit-learn` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      " [[ 0.82797019  0.17511531]\n",
      " [-1.77758033 -0.14285723]\n",
      " [ 0.99219749 -0.38437499]\n",
      " [ 0.27421042 -0.13041721]\n",
      " [ 1.67580142  0.20949846]\n",
      " [ 0.9129491  -0.17528244]\n",
      " [-0.09910944  0.3498247 ]\n",
      " [-1.14457216 -0.04641726]\n",
      " [-0.43804614 -0.01776463]\n",
      " [-1.22382056  0.16267529]]\n",
      "Eigenvalues:\n",
      " [1.28402771 0.0490834 ]\n",
      "Eigenvectors:\n",
      " [[ 0.6778734   0.73517866]\n",
      " [ 0.73517866 -0.6778734 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[2.5, 2.4],\n",
    "                 [0.5, 0.7],\n",
    "                 [2.2, 2.9],\n",
    "                 [1.9, 2.2],\n",
    "                 [3.1, 3.0],\n",
    "                 [2.3, 2.7],\n",
    "                 [2, 1.6],\n",
    "                 [1, 1.1],\n",
    "                 [1.5, 1.6],\n",
    "                 [1.1, 0.9]])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data)\n",
    "\n",
    "print(\"Principal Components:\\n\", principal_components)\n",
    "print(\"Eigenvalues:\\n\", pca.explained_variance_)\n",
    "print(\"Eigenvectors:\\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `principal_components` are the transformed data points.\n",
    "- `pca.explained_variance_` gives the eigenvalues.\n",
    "- `pca.components_` provides the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance and limitations of PCA Analysis <a class=\"anchor\" id=\"limit\"></a>\n",
    "\n",
    "As previously stated, PCA helps simplify complex datasets while retaining important information. It manages this due to the following factors:\n",
    "\n",
    "1. **Dimensional Reduction**: Many datastes have a high number of features, which can be innefficient or even lead to overfitting. In this point PCA reduces the number of variables while preserving key patterns, making the data easier to analyze and visualize.\n",
    "2. **Redundancy and Correlation removal**: Actual data that is used often has multicollinearity features. When PCA is applied, correlated features are transformed into a set of uncorrelated `principal components`. This prevents redundancy and improves the stability of the future model.\n",
    "3. **Handling Noise**: By identifying the `principal components` that cause the most variance in the data, PCA filters out noise. This improves the robustness by removing features considered irrelevant, or less informative.\n",
    "4. **Data Visualization Enhancement**: PCA allows projection into 2D or 3D space, making it possible to visualize patterns, clusters, and trends in the data.\n",
    "5. **Feature Extraction and Engineering**: PCA is used in feature extraction by generating new features (principal components) from existing ones.\n",
    "These new features often capture more meaningful relationships, enhancing the predictive power of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, PCA is recommended in various domains, such as `Image Processing`, `Finance`, `Genomics` and `Recommendation Systems`. \n",
    "\n",
    "Nevertheless, it has its limitations:\n",
    "\n",
    "- It assumes linear realtionships between variables;\n",
    "- Interpretability of transformed components is difficult.\n",
    "- It can possibly not work well when data variance does not capture important patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- **Gilbert Strang - Linear Algebra and Its Applications**\n",
    "- [MIT OpenCourseWare - Lecture 21: Eigenvalues and Eigenvectors](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/resources/lecture-21-eigenvalues-and-eigenvectors/)\n",
    "- [Benyamin Ghojogh, Mark Crowley - Unsupervised and Supervised Principal Component Analysis: Tutorial](https://arxiv.org/abs/1906.03148)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
