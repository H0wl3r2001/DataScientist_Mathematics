{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the theory behind Machine Learning algorithms, which are prevalent in the Data Science field, one must know the concepts of Linear Algebra.\n",
    "\n",
    "This is a field of mathematics applied in many fields of study, since it allows us to efficiently model concepts.\n",
    "\n",
    "In this notebook, we will explore basic linear algebra terms and phenomena, in the eyes of a computer scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "1. [**Scalars and vectors**](#ScalVec)\n",
    "2. [**Matrices and Tensors**](#MatTens)\n",
    "3. [**Transformation Matrix**](#MatTransf)\n",
    "4. [**Machine Learning Applications**](#MLApp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars and Vectors <a id=\"ScalVec\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scalar is a single number, often representing a quantity or measurement (for example, 24). Scalars are the simplest form of data in linear algebra and are used extensively in machine learning to represent weights, biases, and other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector is an ordered list of numbers, which can be thought of as a point in a multi-dimensional space. Vectors are fundamental in machine learning as they are used to represent data points, feature sets, and gradients. \n",
    "\n",
    "In python, using the numpy library, it is simple to define vectors and do operations using scalars and other vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of 10 ones:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      " Vector of 10 numbers that have the same distance from each other, starting with 3 and ending with 15:\n",
      " [ 3.          4.33333333  5.66666667  7.          8.33333333  9.66666667\n",
      " 11.         12.33333333 13.66666667 15.        ]\n",
      "\n",
      " Vector of 10 random numbers between [0, 1):\n",
      " [0.8604645  0.02176455 0.69184775 0.70485581 0.61359383 0.51915531\n",
      " 0.19527979 0.77190631 0.13059284 0.38450186]\n",
      "\n",
      " Vector of 10 random numbers between [1, 10):\n",
      " [2 8 4 7 7 1 7 7 6 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a1 = np.ones(10)\n",
    "print(\"Vector of 10 ones:\\n\", a1)\n",
    "\n",
    "a2 = np.linspace(3, 15, 10)\n",
    "print(\"\\n Vector of 10 numbers that have the same distance from each other, starting with 3 and ending with 15:\\n\", a2)\n",
    "\n",
    "a3 = np.random.rand(10)\n",
    "print(\"\\n Vector of 10 random numbers between [0, 1):\\n\", a3)\n",
    "\n",
    "a4 = np.random.randint(1, 10, size=10)\n",
    "print(\"\\n Vector of 10 random numbers between [1, 10):\\n\", a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a1:\n",
      " [1 2 3 4]\n",
      "\n",
      "Adding the scalar 2 to the vector a1:\n",
      " [3 4 5 6]\n",
      "\n",
      "Subtract a scalar of value 1 to the vector.\n",
      " [0 1 2 3]\n",
      "\n",
      "Multiply the vector with a scalar of value 10:\n",
      " [10 20 30 40]\n",
      "\n",
      "Divide the vector by a scalar of value 5:\n",
      " [0.2 0.4 0.6 0.8]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([1, 2, 3, 4])\n",
    "\n",
    "print(\"Vector a1:\\n\", a1)\n",
    "\n",
    "print(\"\\nAdding the scalar 2 to the vector a1:\\n\", a1 + 2)\n",
    "\n",
    "print(\"\\nSubtract a scalar of value 1 to the vector.\\n\", a1 - 1)\n",
    "\n",
    "print(\"\\nMultiply the vector with a scalar of value 10:\\n\", a1 * 10)\n",
    "\n",
    "print(\"\\nDivide the vector by a scalar of value 5:\\n\", a1 / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices and Tensors <a id=\"MatTens\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices and tensors are higher-dimensional generalizations of vectors and are crucial in the field of machine learning.\n",
    "\n",
    "### Matrices\n",
    "\n",
    "A matrix is a two-dimensional array of numbers, which can be thought of as a collection of vectors. Matrices are used extensively in machine learning to represent datasets, transformation operations, and more. For example, in supervised learning, a dataset is often represented as a matrix where each row corresponds to a data point and each column corresponds to a feature.\n",
    "\n",
    "In python, using the numpy library, it is simple to define matrices and perform operations on them, using matrices, vectors and scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Matrix 2:\n",
      " [[9 8 7]\n",
      " [6 5 4]\n",
      " [3 2 1]]\n",
      "\n",
      "Sum of Matrix 1 and Matrix 2:\n",
      " [[10 10 10]\n",
      " [10 10 10]\n",
      " [10 10 10]]\n",
      "\n",
      "Difference of Matrix 1 and Matrix 2:\n",
      " [[-8 -6 -4]\n",
      " [-2  0  2]\n",
      " [ 4  6  8]]\n"
     ]
    }
   ],
   "source": [
    "# Define matrices\n",
    "matrix1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "matrix2 = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n",
    "\n",
    "print(\"Matrix 1:\\n\", matrix1)\n",
    "print(\"\\nMatrix 2:\\n\", matrix2)\n",
    "\n",
    "# Matrix addition\n",
    "matrix_sum = matrix1 + matrix2\n",
    "print(\"\\nSum of Matrix 1 and Matrix 2:\\n\", matrix_sum)\n",
    "\n",
    "# Matrix subtraction\n",
    "matrix_diff = matrix1 - matrix2\n",
    "print(\"\\nDifference of Matrix 1 and Matrix 2:\\n\", matrix_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Element-wise multiplication of Matrix 1 and Matrix 2:\n",
      " [[ 9 16 21]\n",
      " [24 25 24]\n",
      " [21 16  9]]\n",
      "\n",
      "Dot product of Matrix 1 and Matrix 2:\n",
      " [[ 30  24  18]\n",
      " [ 84  69  54]\n",
      " [138 114  90]]\n",
      "\n",
      "Matrix 1 multiplied by scalar 2:\n",
      " [[ 2  4  6]\n",
      " [ 8 10 12]\n",
      " [14 16 18]]\n",
      "\n",
      "Matrix 1 multiplied by vector [1, 2, 3]:\n",
      " [14 32 50]\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "matrix_product = matrix1 * matrix2\n",
    "print(\"\\nElement-wise multiplication of Matrix 1 and Matrix 2:\\n\", matrix_product)\n",
    "\n",
    "# Matrix dot product\n",
    "matrix_dot_product = np.dot(matrix1, matrix2)\n",
    "print(\"\\nDot product of Matrix 1 and Matrix 2:\\n\", matrix_dot_product)\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar = 2\n",
    "matrix_scalar_product = matrix1 * scalar\n",
    "print(\"\\nMatrix 1 multiplied by scalar 2:\\n\", matrix_scalar_product)\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix_vector_product = np.dot(matrix1, vector)\n",
    "print(\"\\nMatrix 1 multiplied by vector [1, 2, 3]:\\n\", matrix_vector_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication Properties\n",
    "\n",
    "Matrix multiplication is bound to the following properties:\n",
    "\n",
    "1. **Associativity**:\n",
    "    $$\n",
    "    (AB)C = A(BC)\n",
    "    $$\n",
    "    Matrix multiplication is associative, meaning the order in which matrices are multiplied does not affect the result.\n",
    "\n",
    "2. **Distributivity**:\n",
    "    $$\n",
    "    A(B + C) = AB + AC\n",
    "    $$\n",
    "    $$\n",
    "    (A + B)C = AC + BC\n",
    "    $$\n",
    "    Matrix multiplication is distributive over matrix addition.\n",
    "\n",
    "3. **Non-Commutativity**:\n",
    "    $$\n",
    "    AB \\neq BA\n",
    "    $$\n",
    "    In general, matrix multiplication is not commutative, meaning the order of multiplication matters.\n",
    "\n",
    "4. **Identity Matrix**:\n",
    "    $$\n",
    "    AI = IA = A\n",
    "    $$\n",
    "    Multiplying any matrix \\(A\\) by the identity matrix \\(I\\) results in the original matrix \\(A\\).\n",
    "\n",
    "    * **Inverse**\n",
    "        $$\n",
    "        A*1/A = I\n",
    "        $$\n",
    "        If the matrix \\(A\\) is multiplied by its Inverse matrix \\(1/A\\), it results in the Identity matrix. Note that not every matrix has an inverse.\n",
    "\n",
    "5. **Zero Matrix**:\n",
    "    $$\n",
    "    A0 = 0A = 0\n",
    "    $$\n",
    "    Multiplying any matrix \\(A\\) by the zero matrix \\(0\\) results in the zero matrix.\n",
    "\n",
    "6. **Transpose**:\n",
    "    $$\n",
    "    (AB)^T = B^T A^T\n",
    "    $$\n",
    "    The transpose of a product of two matrices is the product of their transposes in reverse order.\n",
    "\n",
    "7. **Scalar Multiplication**:\n",
    "    $$\n",
    "    c(AB) = (cA)B = A(cB)\n",
    "    $$\n",
    "    Scalar multiplication can be distributed across matrix multiplication.\n",
    "\n",
    "Understanding these properties is essential for performing and simplifying matrix operations in various applications, including machine learning and computer graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "A tensor is a multi-dimensional array of numbers. Depending on the number of indexes, they can be a vector (1 index), matrix (2 indexes), and beyond the third-order tensors (3 indexes, where the 1st points to the row, 2nd to the column and 3rd to the axis), we have higher-order tensors. This makes them a generalization of the previous concepts. This is important for machine learning porpouses because:\n",
    "\n",
    "1. **Data Representation**: Tensors are used to represent data in machine learning. For example, a color image can be represented as a 3-dimensional tensor with dimensions corresponding to height, width, and color channels (RGB).\n",
    "\n",
    "2. **Model Parameters**: In neural networks, tensors are used to represent the weights and biases of the model. These parameters are updated during the training process to minimize the loss function.\n",
    "\n",
    "3. **Operations and Transformations**: Tensors allow for efficient mathematical operations and transformations. Libraries like TensorFlow and PyTorch are built around tensor operations, enabling efficient computation on GPUs and other hardware accelerators.\n",
    "\n",
    "4. **Batch Processing**: Tensors enable batch processing of data, which is essential for training machine learning models. By processing multiple data points simultaneously, tensors help in speeding up the training process.\n",
    "\n",
    "5. **Flexibility**: Tensors provide a flexible way to handle different types of data, including structured and unstructured data. This flexibility is crucial for building complex machine learning models that can handle diverse data sources.\n",
    "\n",
    "In summary, tensors are fundamental to the field of machine learning, specially Deep Learning, as they provide a powerful and flexible way to represent and manipulate data and model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Matrix <a id=\"MatTransf\"></a>\n",
    "\n",
    "A transformation matrix is a matrix used to perform linear transformations on vectors in a given space. These transformations include operations such as translation, scaling, rotation, and shearing. Transformation matrices are widely used in computer graphics, robotics, and machine learning.\n",
    "\n",
    "### Types of Transformations\n",
    "\n",
    "1. **Translation**: This moves every point of an object by the same distance in a given direction. In 2D, the translation matrix is:\n",
    "    $$\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & t_x \\\\\n",
    "    0 & 1 & t_y \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    where t_x and t_y are the translation distances along the x and y axes, respectively.\n",
    "\n",
    "2. **Scaling**: This changes the size of an object. In 2D, the scaling matrix is:\n",
    "    $$\n",
    "    \\begin{bmatrix}\n",
    "    s_x & 0 & 0 \\\\\n",
    "    0 & s_y & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    where s_x and s_y are the scaling factors along the x and y axes, respectively.\n",
    "\n",
    "3. **Rotation**: This rotates an object around the origin. In 2D, the rotation matrix is:\n",
    "    $$\n",
    "    \\begin{bmatrix}\n",
    "    \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\\n",
    "    \\sin(\\theta) & \\cos(\\theta) & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    where theta is the angle of rotation.\n",
    "\n",
    "4. **Shearing**: This slants the shape of an object. In 2D, the shearing matrix is:\n",
    "    $$\n",
    "    \\begin{bmatrix}\n",
    "    1 & sh_x & 0 \\\\\n",
    "    sh_y & 1 & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    where sh_x and sh_y are the shearing factors along the x and y axes, respectively.\n",
    "\n",
    "#### Combining Transformations\n",
    "\n",
    "Multiple transformations can be combined into a single transformation matrix by matrix multiplication. The order of multiplication is important, as matrix multiplication is not commutative.\n",
    "\n",
    "## Applications in Machine Learning <a id=\"MLApp\"></a>\n",
    "\n",
    "In machine learning, transformation matrices are used in various applications, such as:\n",
    "\n",
    "- **Data Augmentation**: Applying transformations to training data to increase the diversity of the dataset.\n",
    "- **Feature Engineering**: Transforming features to improve the performance of machine learning models.\n",
    "- **Neural Networks**: Applying linear transformations in the layers of neural networks.\n",
    "\n",
    "Understanding transformation matrices is crucial for effectively manipulating and interpreting data in machine learning and other fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://builtin.com/data-science/basic-linear-algebra-deep-learning\n",
    "- https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
